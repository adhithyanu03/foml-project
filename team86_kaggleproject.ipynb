{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the original training and test data\n",
        "train_data = pd.read_csv('iith_foml_2023_train.csv')\n",
        "test_data = pd.read_csv('iith_foml_2023_test.csv')\n",
        "\n",
        "# Assuming the target column is named 'Target Variable (Discrete)'\n",
        "X_train = train_data.drop('Target Variable (Discrete)', axis=1)\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Impute missing values in the training data\n",
        "imputer = SimpleImputer()\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Train the Naive Bayes model (assuming Gaussian Naive Bayes for simplicity)\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Train the Support Vector Machine (SVM) model\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Train the k-Nearest Neighbors (kNN) model\n",
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Impute missing values in the test data\n",
        "X_test_imputed = imputer.transform(test_data)\n",
        "\n",
        "# Make predictions using each model\n",
        "rf_predictions = rf_model.predict(X_test_imputed)\n",
        "xgb_predictions = xgb_model.predict(X_test_imputed)\n",
        "nb_predictions = nb_model.predict(X_test_imputed)\n",
        "svm_predictions = svm_model.predict(X_test_imputed)\n",
        "knn_predictions = knn_model.predict(X_test_imputed)\n",
        "\n",
        "# Create separate DataFrames for each model's predictions\n",
        "rf_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': rf_predictions})\n",
        "xgb_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': xgb_predictions})\n",
        "nb_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': nb_predictions})\n",
        "svm_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': svm_predictions})\n",
        "knn_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': knn_predictions})\n",
        "\n",
        "# Save predictions to separate CSV files\n",
        "rf_df.to_csv('rf_predictions.csv', index=False)#0.5\n",
        "xgb_df.to_csv('xgb_predictions.csv', index=False)#0.5\n",
        "nb_df.to_csv('nb_predictions.csv', index=False)#o.27\n",
        "svm_df.to_csv('svm_predictions.csv', index=False)#0.26\n",
        "knn_df.to_csv('knn_predictions.csv', index=False)#0.32\n"
      ],
      "metadata": {
        "id": "SO9qEjOynNtW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "all these  predictions are submitted in kaggle and found their accuracy  and two of them random forest and XGB predictions is better than others\n",
        "with score of ~50% so we will use this for further process"
      ],
      "metadata": {
        "id": "lGpMvqQr72kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Load the original training and test data\n",
        "train_data = pd.read_csv('iith_foml_2023_train.csv')\n",
        "test_data = pd.read_csv('iith_foml_2023_test.csv')\n",
        "\n",
        "# Combine the training and test data\n",
        "combined_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
        "\n",
        "# Assuming the target column is named 'Target Variable (Discrete)'\n",
        "y_train = train_data['Target Variable (Discrete)']\n",
        "\n",
        "# Drop the target column for NN clustering\n",
        "combined_data_for_nn = combined_data.drop('Target Variable (Discrete)', axis=1)\n",
        "\n",
        "# Impute missing values in the combined data\n",
        "imputer = SimpleImputer()\n",
        "X_combined_imputed = imputer.fit_transform(combined_data_for_nn)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_combined_standardized = scaler.fit_transform(X_combined_imputed)\n",
        "\n",
        "# Split the data back into training and test sets\n",
        "X_train_standardized = X_combined_standardized[:len(train_data)]\n",
        "X_test_standardized = X_combined_standardized[len(train_data):]\n",
        "\n",
        "# Create and train the neural network model\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=1000, alpha=0.001, random_state=42)\n",
        "\n",
        "nn_model.fit(X_train_standardized, y_train)\n",
        "\n",
        "# Perform NN clustering on the test data\n",
        "test_predictions = nn_model.predict(X_test_standardized)\n",
        "\n",
        "# Convert predictions to strings\n",
        "test_predictions_str = test_predictions.astype(str)\n",
        "\n",
        "# Create a DataFrame with 'ID' and 'Category_NN' columns for test predictions\n",
        "result_df = pd.DataFrame({'ID': range(1, len(test_data) + 1), 'Category': test_predictions_str})\n",
        "\n",
        "# Save the NN test predictions to a CSV file\n",
        "result_df.to_csv('nn_test_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4pxEzIC_L9Jb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural netowrk prediction also performs good with ~55% accuracy  "
      ],
      "metadata": {
        "id": "K8kBtpz-8TKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used 6 prediction algo out of which 3 of them performed best of around ~ 55 %,52% 50 % consistently those are Neural networks, randomforest and XGB"
      ],
      "metadata": {
        "id": "zXQ-3EDM5_el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**randomized sampling from 3 input predictions**\n",
        "\n",
        "\n",
        "prediction algo~ randomized sampling + accuracy estimator\n",
        "\n",
        "usualy accuracy estimator can be definead as a function ,where as we used kaggle submissions as accuracy estimation"
      ],
      "metadata": {
        "id": "VY547FynCQQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def merge_and_update(input_files, output_file, iterations):\n",
        "    # Read the initial CSV files\n",
        "    csv_files = [pd.read_csv(file) for file in input_files]\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Merge the dataframes on the 'ID' column\n",
        "        merged_df = pd.merge(csv_files[0], csv_files[1], on='ID')\n",
        "        merged_df = pd.merge(merged_df, csv_files[2], on='ID')\n",
        "\n",
        "        # Randomly select a value from the three categories for each ID\n",
        "        merged_df['Category'] = merged_df.apply(lambda row: random.choice([row['Category_x'], row['Category_y'], row['Category']]), axis=1)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        merged_df = merged_df[['ID', 'Category']]\n",
        "\n",
        "        # Update one of the input files with the new merged and randomized data\n",
        "        updated_file_index = random.randint(0, 2)\n",
        "        csv_files[updated_file_index] = merged_df.copy()\n",
        "\n",
        "    # Save the final result to the specified output file\n",
        "    merged_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Run the convergence process for 10 iterations (adjust as needed)\n",
        "merge_and_update(['rf_predictions.csv', 'xgb_predictions.csv', 'nn_test_predictions.csv'], 'result1.csv', iterations=1000)\n"
      ],
      "metadata": {
        "id": "cqOkosP0-YBi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**intuition:**\n",
        "\n",
        "\n",
        "take 3 input csv file and produce a randomized csv made up of prediction values from these file replace the worst scored input with a better scored output (we performed this for a number of time to get better scores )"
      ],
      "metadata": {
        "id": "eMwE0jLoBy8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the best result we got  yet from this randomised selection algo - result2.csv of 68.3%, result4.csv- 69% and result1.csv 68.8%"
      ],
      "metadata": {
        "id": "wG_jlGwH5jyL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52naRo-tEFf6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: this prediction took a lot of trial and error to correctly predict , get a better prediction accuracy but given a no of iterations of this method will provide better results ( we got ours after 3 rd iteration after which score seemed to decrease )\n"
      ],
      "metadata": {
        "id": "0whMP9Gx7C_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next process would be replace the csv file with lower accuracy with better one and observe the randomized algo to provide with a predicted csv file which might have better accuracy than others"
      ],
      "metadata": {
        "id": "4PMfj3sO6upm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "merge_and_update(['result2.csv', 'result1.csv', 'nn_test_predictions.csv'], 'result5.csv', iterations=1000)\n"
      ],
      "metadata": {
        "id": "D68909VT2j2c"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "result5.csv- 0.72"
      ],
      "metadata": {
        "id": "dJ334ICK5JB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_and_update(['result5.csv', 'result1.csv', 'nn_test_predictions.csv'], 'result6.csv', iterations=1000)"
      ],
      "metadata": {
        "id": "UeTBqF163L-p"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "result6.csv-0.63"
      ],
      "metadata": {
        "id": "ZiCfgAVM5M8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_and_update(['result5.csv', 'result1.csv', 'result4.csv'], 'result7.csv', iterations=1000)"
      ],
      "metadata": {
        "id": "nS53oU8h5CeH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why this randomised algo works ?**\n",
        "\n",
        "we initialy took 3 csv files with accuracy >51% so randomly picking from these 3 files might provide higher accuracy than other or lower than others\n",
        "\n",
        "\n",
        "by iteratively replacing the lowest scored value we actually improving prediction accuracy , if the prediction accuracy is worst than the 3 inputs just ignore the result and iterate once again\n",
        "\n",
        "\n",
        "Note : this method tends to reach a maximum score above which prediction accuracy cant be acquired and that for our case is 72.096% this algo may not provide the same output as us but it will give better score based on no of submissions( we executed and submitted 55 times to improve score ˙◠˙)"
      ],
      "metadata": {
        "id": "SsYRfPzuAz8T"
      }
    }
  ]
}